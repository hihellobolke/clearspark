import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark._
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import org.json4s._
import org.json4s.jackson.JsonMethods._
implicit val formats = DefaultFormats


object ClearSpacer {
  def main(args: Array[String]) {
    //if (args.length < 2) {
      //System.err.println("Usage: RawNetInput <hostname> <port>")
      //System.exit(1)
    //val args: Array[String] = Array("127.0.0.1", "9999")
    //}

    //StreamingExamples.setStreamingLogLevels()

    // Create the context with a 1 second batch size
    val master = "spark://solo-kumbu.local:7077"
    val master = "spark://solo-kumbu:7077"
    val sparkConf = new SparkConf().setAppName("ClearSpacer").setMaster(master)
    val sc = new SparkContext(sparkConf)

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.

    val nodeFile = sc.textFile("/home/epiclulz/workspace/bitbucket/clearspark/nodes.json")
    val relFile = sc.textFile("/home/epiclulz/workspace/bitbucket/clearspark/rels.json")
    val lines = ssc.socketTextStream("localhost", "9999".toInt, StorageLevel.MEMORY_AND_DISK_SER)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}


case class NodeFromJson(
                        hash: Long,
                        name: String,
                        timestamp: Long,
                        properties: Map[String, String],
                        seen: Int,
                        node_type: String,
                        id: Long)

case class RelFromJson(
                        src_id: Long,
                        dst_id: Long,
                        hash: Long,
                        name: String,
                        timestamp: Long,
                        properties: Map[String, String],
                        seen: Int,
                        rel_type: String,
                        id: Long)


def NodeParse(s: String): (Long, (String, String, Int, Long, Long, Map[String, String])) = {
    val p = parse(s).extract[NodeFromJson];
    return (p.id, (p.name, p.node_type, p.seen, p.timestamp, p.id, p.properties))
}

def RelParse(s: String):
Edge[(String, String, Int, Long, Long, Long, Long, Map[String, String])] = {
    val p = parse(s).extract[RelFromJson];
    return Edge(p.src_id, p.dst_id,
            (p.name, p.rel_type, p.seen, p.timestamp,
             p.id, p.src_id, p.dst_id, p.properties))
}


val vertexes: RDD[(VertexId,
                   (String, String, Int, Long, Long, Map[String,String]))] =
                   nodeFile.map(a => NodeParse(a))

val relationships: RDD[Edge[(String, String,
                             Int, Long, Long,
                             Long, Long, Map[String, String])]] =
                    relFile.map(a => RelParse(a))

val graph = Graph(vertexes, relationships)
